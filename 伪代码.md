# rl_train.py 的伪代码

```
Algorithm: Deep Q-Learning with Experience Replay for Photometric Stereo

Initialize:
    Q(s,a) with random weights θ
    Q̂(s,a) with weights θ⁻ = θ
    Experience replay buffer D

for episode = 1 to M do
    Initialize state s₁
    for t = 1 to T do
        With probability ε select random action aₜ
        Otherwise select aₜ = argmax_a Q(sₜ, a; θ)
        Execute action aₜ, observe reward rₜ and next state sₜ₊₁
        Store transition (sₜ, aₜ, rₜ, sₜ₊₁) in D
        Sample random minibatch of transitions (sⱼ, aⱼ, rⱼ, sⱼ₊₁) from D
        Set yⱼ = rⱼ + γ max_a' Q̂(sⱼ₊₁, a'; θ⁻)
        Perform gradient descent step on (yⱼ - Q(sⱼ, aⱼ; θ))² with respect to θ
        Every C steps reset Q̂ = Q
    end for
end for

where:
    s: state (images and light directions)
    a: action (selection of next light direction)
    r: reward (improvement in surface normal estimation)
    γ: discount factor
    ε: exploration rate
    θ, θ⁻: weights of Q and target networks respectively
    C: frequency of target network update
```

1. 使用两个Q网络（当前网络和目标网络）
2. Experience replay机制
3. ε-greedy策略用于动作选择
4. 目标Q值的计算使用目标网络
5. 定期更新目标网络

```
State representation:
    s = (I₁, ..., Iₙ, L₁, ..., Lₙ), where:
        Iᵢ: captured image under light direction Lᵢ
        Lᵢ: light direction for i-th capture

Action:
    a ∈ {1, ..., K}, where K is the number of possible light directions

Reward:
    r = improvement in estimated surface normal accuracy
```


Q: 我有1个问题：为什么 不全都使用 select random action? 为什么要让 aₜ = argmax_a Q(sₜ, a; θ) 参与进来？我以为 Store transition 只是为了制作数据

A:您提出了一个非常好的问题。这涉及到强化学习中的一个核心概念：探索与利用的平衡（Exploration vs. Exploitation trade-off）。让我解释一下为什么我们需要同时使用随机动作和基于Q值的动作选择：

1. 探索 (Exploration):
   - 随机选择动作（With probability ε select random action aₜ）允许智能体探索环境，发现可能的新策略。
   - 这有助于防止智能体陷入局部最优解。

2. 利用 (Exploitation):
   - 选择Q值最大的动作（aₜ = argmax_a Q(sₜ, a; θ)）允许智能体利用已学到的知识。
   - 这确保智能体能够逐步改进其策略，朝着最优策略方向发展。

3. 在线学习:
   - 虽然我们确实在存储转换以供后续训练，但强化学习通常是一个在线学习过程。
   - 智能体在与环境交互时同时学习和改进其策略。

4. 策略评估和改进:
   - 通过执行当前认为最佳的动作，我们可以评估当前策略的效果。
   - 这为Q函数提供了更有意义的更新信息。

5. 数据多样性:
   - 结合随机动作和基于Q值的动作，可以生成更多样化的数据。
   - 这有助于避免数据集中的偏差，提高学习的稳定性和泛化能力。

6. 逐步收敛:
   - 随着学习的进行，我们通常会逐渐减少随机动作的概率（ε逐渐减小）。
   - 这允许智能体在初期大量探索，后期更多地利用学到的知识。

在您的特定应用（光度立体视觉）中，这种方法特别重要：

- 探索阶段可能发现一些意外有效的光照方向。
- 利用阶段则确保算法能够逐步优化光照方向的选择。

总之，结合随机动作和基于Q值的动作选择不仅是为了制作训练数据，更是强化学习算法本身的核心机制，用于在探索新可能性和利用已知信息之间取得平衡，从而达到最优策略。

Q: 不对，我的意思是为什么不能 改成 一部分概率随机选择，一部分概率选择 greedy？这样子不是更好吗? 为什么要让 Q-Net 来预测 action 并且保存到 buffer 里面?

A: 这个算法实际上是 online-learning 所以要一步一步的学习：

您提出了一个很好的观察。让我解释一下这段代码中的策略选择逻辑：

1. Q网络预测（默认选择）:
   ```python
   action = np.argmax(q_vals.mean(axis=(0, 1)).flatten()[env.mesh_action_ids(_n_light_axix)])
   ```
   这是基于当前Q网络的预测来选择动作。

2. ε-贪心策略:
   ```python
   if sel < eps:
       if os.environ.get("RLPS_ABLATION") == "3":
           action = rng.integers(env.action_space()) # 随机选择动作
       else:
           action = env.action_greedy() # 贪婪选择动作
   ```
   这里实现了ε-贪心策略，但有一个条件分支。

3. 最终动作选择:
   ```python
   action = env.action_around(action)
   ```
   这一步可能是对选定动作的微调。

关于您的问题，为什么使用这三种可能的actions而不仅仅是random和greedy：

1. 学习和利用Q网络:
   - 使用Q网络的预测作为默认选择允许算法利用已学到的知识。
   - 这对于策略的持续改进是必要的。

2. 实验设置和消融研究:
   - 代码中的条件分支（RLPS_ABLATION == "3"）暗示这可能是为了进行消融研究。
   - 这允许研究者比较不同策略的效果。

3. 平衡探索和利用:
   - Q网络预测提供了一种"软"探索方式，介于纯随机和纯贪婪之间。
   - 这可能有助于在特定问题上获得更好的性能。

4. 适应性:
   - 结合Q网络预测的方法可能更适应复杂或动态变化的环境。

5. 渐进式学习:
   - 随着训练的进行，Q网络的预测会越来越准确，逐渐接近最优策略。

然而，您的观点也有道理。在某些情况下，简单的ε-贪心策略（只使用random和greedy）可能足够好，而且更容易实现和理解。这种方法的优点包括：

- 简单性：更容易实现和调试。
- 计算效率：不需要额外的Q值计算。
- 可解释性：行为更直观，易于分析。
```

这种简化可能会使代码更清晰，同时仍然保持探索和利用之间的平衡。但是，请记住要进行实验比较，以确保这种简化不会显著影响您的学习效果。


Q: 最后预测出来的 Q-value tensor 的 数值是什么意思？为什么有的地方数值高，有的地方少？

很好的问题！Q值（Q-values）代表了在特定状态下采取特定动作的预期未来累积奖励。让我们深入解析一下：

Q值的含义：
1. 定义：Q(s,a) 表示在状态 s 下采取动作 a 的预期累积折扣奖励。
2. 解释：它预测了从当前状态开始，采取特定动作后，长期内能获得的总奖励。

在您的光度立体视觉应用中：
- 状态 (s)：当前已捕获的图像和光照方向。
- 动作 (a)：下一个要选择的光照方向。
- Q值：预测选择特定光照方向后，最终能获得的表面法线估计质量改善。

Q值张量的解释：
```python
q_vals = qnet(x_images, x_dirs)[0, -1, ...].cpu().numpy()
```
这个张量可能有多个维度，每个维度代表不同的方面：

1. 空间维度：如果 q_vals 包含空间信息，高值区域可能表示：
   - 图像中那些区域选择特定光照方向会带来更大改善。
   - 可能对应于物体表面的特定几何特征（如边缘、曲率高的区域）。

2. 动作维度：如果 q_vals 包含不同动作的预测：
   - 高值表示该动作（光照方向）预期会带来更大的改善。
   - 低值表示该动作可能不太有利。

3. 时间或序列维度：如果存在这样的维度：
   - 可能表示在捕获序列的不同阶段，不同动作的相对重要性。

为什么有的地方数值高，有的地方低？
1. 物体几何特性：
   - 复杂区域（如有细节的部分）可能有更高的 Q 值，因为额外的图像可能提供更多信息。
   - 平坦区域可能有较低的 Q 值，因为额外的图像可能不会提供太多新信息。

2. 当前信息状态：
   - 已经有充分信息的区域可能有较低的 Q 值。
   - 信息不足的区域可能有较高的 Q 值，表示需要更多数据。

3. 光照方向的影响：
   - 某些光照方向可能对特定表面方向特别有效，导致较高的 Q 值。
   - 而对于其他方向，可能效果不佳，导致较低的 Q 值。

4. 学习过程的影响：
   - 高 Q 值可能表示算法已经学到这些选择通常会带来好结果。
   - 低 Q 值可能表示这些选择在过去的经验中效果不佳。

5. 探索-利用权衡：
   - 较新或较少探索的状态-动作对可能有较高的 Q 值，以鼓励探索。

6. 奖励结构：
   - Q 值的分布反映了您定义的奖励结构。高 Q 值区域对应于能带来高奖励的状态和动作。

理解和可视化这些 Q 值可以帮助您洞察算法的决策过程，以及它如何理解您的光度立体视觉问题。通过分析 Q 值的分布，您可以了解算法认为哪些光照方向和图像区域最重要，这可能会为改进算法或问题设置提供宝贵的见解。
