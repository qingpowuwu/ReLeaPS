# rl_train.py 的伪代码

```
Algorithm: Deep Q-Learning with Experience Replay for Photometric Stereo

Initialize:
    Q(s,a) with random weights θ
    Q̂(s,a) with weights θ⁻ = θ
    Experience replay buffer D

for episode = 1 to M do
    Initialize state s₁
    for t = 1 to T do
        With probability ε select random action aₜ
        Otherwise select aₜ = argmax_a Q(sₜ, a; θ)
        Execute action aₜ, observe reward rₜ and next state sₜ₊₁
        Store transition (sₜ, aₜ, rₜ, sₜ₊₁) in D
        Sample random minibatch of transitions (sⱼ, aⱼ, rⱼ, sⱼ₊₁) from D
        Set yⱼ = rⱼ + γ max_a' Q̂(sⱼ₊₁, a'; θ⁻)
        Perform gradient descent step on (yⱼ - Q(sⱼ, aⱼ; θ))² with respect to θ
        Every C steps reset Q̂ = Q
    end for
end for

where:
    s: state (images and light directions)
    a: action (selection of next light direction)
    r: reward (improvement in surface normal estimation)
    γ: discount factor
    ε: exploration rate
    θ, θ⁻: weights of Q and target networks respectively
    C: frequency of target network update
```

1. 使用两个Q网络（当前网络和目标网络）
2. Experience replay机制
3. ε-greedy策略用于动作选择
4. 目标Q值的计算使用目标网络
5. 定期更新目标网络

```
State representation:
    s = (I₁, ..., Iₙ, L₁, ..., Lₙ), where:
        Iᵢ: captured image under light direction Lᵢ
        Lᵢ: light direction for i-th capture

Action:
    a ∈ {1, ..., K}, where K is the number of possible light directions

Reward:
    r = improvement in estimated surface normal accuracy
```


Q: 我有1个问题：为什么 不全都使用 select random action? 为什么要让 aₜ = argmax_a Q(sₜ, a; θ) 参与进来？我以为 Store transition 只是为了制作数据

A:您提出了一个非常好的问题。这涉及到强化学习中的一个核心概念：探索与利用的平衡（Exploration vs. Exploitation trade-off）。让我解释一下为什么我们需要同时使用随机动作和基于Q值的动作选择：

1. 探索 (Exploration):
   - 随机选择动作（With probability ε select random action aₜ）允许智能体探索环境，发现可能的新策略。
   - 这有助于防止智能体陷入局部最优解。

2. 利用 (Exploitation):
   - 选择Q值最大的动作（aₜ = argmax_a Q(sₜ, a; θ)）允许智能体利用已学到的知识。
   - 这确保智能体能够逐步改进其策略，朝着最优策略方向发展。

3. 在线学习:
   - 虽然我们确实在存储转换以供后续训练，但强化学习通常是一个在线学习过程。
   - 智能体在与环境交互时同时学习和改进其策略。

4. 策略评估和改进:
   - 通过执行当前认为最佳的动作，我们可以评估当前策略的效果。
   - 这为Q函数提供了更有意义的更新信息。

5. 数据多样性:
   - 结合随机动作和基于Q值的动作，可以生成更多样化的数据。
   - 这有助于避免数据集中的偏差，提高学习的稳定性和泛化能力。

6. 逐步收敛:
   - 随着学习的进行，我们通常会逐渐减少随机动作的概率（ε逐渐减小）。
   - 这允许智能体在初期大量探索，后期更多地利用学到的知识。

在您的特定应用（光度立体视觉）中，这种方法特别重要：

- 探索阶段可能发现一些意外有效的光照方向。
- 利用阶段则确保算法能够逐步优化光照方向的选择。

总之，结合随机动作和基于Q值的动作选择不仅是为了制作训练数据，更是强化学习算法本身的核心机制，用于在探索新可能性和利用已知信息之间取得平衡，从而达到最优策略。

Q: 不对，我的意思是为什么不能 改成 一部分概率随机选择，一部分概率选择 greedy？这样子不是更好吗? 为什么要让 Q-Net 来预测 action 并且保存到 buffer 里面?

A: 这个算法实际上是 online-learning 所以要一步一步的学习：

您提出了一个很好的观察。让我解释一下这段代码中的策略选择逻辑：

1. Q网络预测（默认选择）:
   ```python
   action = np.argmax(q_vals.mean(axis=(0, 1)).flatten()[env.mesh_action_ids(_n_light_axix)])
   ```
   这是基于当前Q网络的预测来选择动作。

2. ε-贪心策略:
   ```python
   if sel < eps:
       if os.environ.get("RLPS_ABLATION") == "3":
           action = rng.integers(env.action_space()) # 随机选择动作
       else:
           action = env.action_greedy() # 贪婪选择动作
   ```
   这里实现了ε-贪心策略，但有一个条件分支。

3. 最终动作选择:
   ```python
   action = env.action_around(action)
   ```
   这一步可能是对选定动作的微调。

关于您的问题，为什么使用这三种可能的actions而不仅仅是random和greedy：

1. 学习和利用Q网络:
   - 使用Q网络的预测作为默认选择允许算法利用已学到的知识。
   - 这对于策略的持续改进是必要的。

2. 实验设置和消融研究:
   - 代码中的条件分支（RLPS_ABLATION == "3"）暗示这可能是为了进行消融研究。
   - 这允许研究者比较不同策略的效果。

3. 平衡探索和利用:
   - Q网络预测提供了一种"软"探索方式，介于纯随机和纯贪婪之间。
   - 这可能有助于在特定问题上获得更好的性能。

4. 适应性:
   - 结合Q网络预测的方法可能更适应复杂或动态变化的环境。

5. 渐进式学习:
   - 随着训练的进行，Q网络的预测会越来越准确，逐渐接近最优策略。

然而，您的观点也有道理。在某些情况下，简单的ε-贪心策略（只使用random和greedy）可能足够好，而且更容易实现和理解。这种方法的优点包括：

- 简单性：更容易实现和调试。
- 计算效率：不需要额外的Q值计算。
- 可解释性：行为更直观，易于分析。
```

这种简化可能会使代码更清晰，同时仍然保持探索和利用之间的平衡。但是，请记住要进行实验比较，以确保这种简化不会显著影响您的学习效果。

